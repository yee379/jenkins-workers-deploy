apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.k8s.io/component: agent
    app.k8s.io/instance: ytl
    app.k8s.io/name: agent-ldfc
    app.k8s.io/part-of: jenkins
    app.k8s.io/version: 1.0.0
  name: usdf-agent-ldfc-ytl
spec:
  podManagementPolicy: Parallel
  replicas: 1 
  selector:
    matchLabels:
      app.k8s.io/component: agent
      app.k8s.io/instance: ytl
      app.k8s.io/name: agent-ldfc
      app.k8s.io/part-of: jenkins
      app.k8s.io/version: 1.0.0
  serviceName: agent-ldfc
  template:
    metadata:
      labels:
        app.k8s.io/component: agent
        app.k8s.io/instance: ytl
        app.k8s.io/name: agent-ldfc
        app.k8s.io/part-of: jenkins
        app.k8s.io/version: 1.0.0
    spec:
      containers:
      - name: curl
        image: curlimages/curl
        command: ["/bin/sleep", "3650d"]
        imagePullPolicy: IfNotPresent
      - args:
        - --host=tcp://localhost:2375
        - --mtu=1376
        command:
        - /usr/local/bin/dockerd
        env:
        - name: DOCKER_HOST
          value: tcp://localhost:2375
        image: lsstsqre/dind:18.09.5
        imagePullPolicy: Always
        livenessProbe:
          exec:
            command:
            - wget
            - --spider
            - -q
            - http://localhost:2375/_ping
          failureThreshold: 2
          initialDelaySeconds: 5
          periodSeconds: 5
          successThreshold: 1
          timeoutSeconds: 1
        name: dind
        readinessProbe:
          exec:
            command:
            - wget
            - --spider
            - -q
            - http://localhost:2375/_ping
          failureThreshold: 2
          initialDelaySeconds: 5
          periodSeconds: 5
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          limits:
            cpu: "8"
            memory: 32Gi
          requests:
            cpu: "8"
            memory: 32Gi
        securityContext:
          allowPrivilegeEscalation: true
          privileged: true
          readOnlyRootFilesystem: false
          runAsGroup: 0
          runAsNonRoot: false
          runAsUser: 0
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /var/lib/docker
          name: docker-graph-storage
        - mountPath: /j
          name: ws
      - command:
        - sh
        - -c
        - while true; do /usr/local/bin/docker-gc; sleep $GRACE_PERIOD_SECONDS; done
        env:
        - name: DOCKER_HOST
          value: tcp://localhost:2375
        - name: GRACE_PERIOD_SECONDS
          value: "3600"
        - name: MINIMUM_IMAGES_TO_SAVE
          value: "5"
        - name: REMOVE_VOLUMES
          value: "1"
        - name: FORCE_CONTAINER_REMOVAL
          value: "1"
        - name: FORCE_IMAGE_REMOVAL
          value: "1"
        image: lsstsqre/docker-gc:latest
        imagePullPolicy: Always
        name: docker-gc
        resources:
          limits:
            cpu: 500m
            memory: 512Mi
          requests:
            cpu: 500m
            memory: 512Mi
        securityContext:
          allowPrivilegeEscalation: true
          privileged: false
          readOnlyRootFilesystem: false
          runAsGroup: 0
          runAsNonRoot: false
          runAsUser: 0
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      - env:
#        - name: HTTP_PROXY
#          value: http://sdfproxy.sdf.slac.stanford.edu:3128
#        - name: HTTPS_PROXY
#          value: http://sdfproxy.sdf.slac.stanford.edu:3128
#        - name: http_proxy
#          value: http://sdfproxy.sdf.slac.stanford.edu:3128
#        - name: https_proxy
#          value: http://sdfproxy.sdf.slac.stanford.edu:3128
        - name: JAVA_ARGS
          value: "-javaagent:/usr/share/jenkins/jmx_prometheus_javaagent-0.11.0.jar=localhost:8080:/usr/share/jenkins/jmx_exporter.yaml"
# -Dhttp.proxyHost=sdfproxy.sdf.slac.stanford.edu -Dhttps.proxyHost=sdfproxy.sdf.slac.stanford.edu -Dhttp.proxyPort=3128 -Dhttps.proxyPort=3128 -Dhttp.nonProxyHosts=127.0.0.0/8,192.168.0.0/16,10.0.0.0/8,.slac.stanford.edu"
#        - name: NO_PROXY
#          value: "127.0.0.1,localhost,127.0.0.0/8,192.168.0.0/16,10.0.0.0/8,.slac.stanford.edu"
#        - name: no_proxy
#          value: "127.0.0.1,localhost,127.0.0.0/8,192.168.0.0/16,10.0.0.0/8,.slac.stanford.edu"
        - name: DOCKER_HOST
          value: tcp://localhost:2375
        - name: JSWARM_MASTER_URL
          value: https://ci.lsst.codes
        - name: JSWARM_MODE
          value: normal
        - name: JSWARM_LABELS
          value: docker ldfc
        - name: JSWARM_EXECUTORS
          value: "1"
        - name: JSWARM_AGENT_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        - name: JSWARM_DISABLE_CLIENTS_UNIQUE_ID
          value: "true"
        - name: JSWARM_DELETE_EXISTING_CLIENTS
          value: "true"
        - name: JSWARM_USERNAME
          valueFrom:
            secretKeyRef:
              key: JSWARM_USERNAME
              name: agent-ldfc
        - name: JSWARM_PASSWORD
          valueFrom:
            secretKeyRef:
              key: JSWARM_PASSWORD
              name: agent-ldfc
        - name: K8S_NODE_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.nodeName
        - name: K8S_POD_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: K8S_POD_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.podIP
        - name: K8S_SWARM_REQUESTS_CPU
          valueFrom:
            resourceFieldRef:
              containerName: swarm
              divisor: "0"
              resource: requests.cpu
        - name: K8S_SWARM_LIMITS_CPU
          valueFrom:
            resourceFieldRef:
              containerName: swarm
              divisor: "0"
              resource: limits.cpu
        - name: K8S_SWARM_REQUESTS_MEMORY_GI
          valueFrom:
            resourceFieldRef:
              containerName: swarm
              divisor: 1Gi
              resource: requests.memory
        - name: K8S_SWARM_LIMITS_MEMORY_GI
          valueFrom:
            resourceFieldRef:
              containerName: swarm
              divisor: 1Gi
              resource: limits.memory
        - name: K8S_DIND_REQUESTS_CPU
          valueFrom:
            resourceFieldRef:
              containerName: dind
              divisor: "0"
              resource: requests.cpu
        - name: K8S_DIND_LIMITS_CPU
          valueFrom:
            resourceFieldRef:
              containerName: dind
              divisor: "0"
              resource: limits.cpu
        - name: K8S_DIND_REQUESTS_MEMORY_GI
          valueFrom:
            resourceFieldRef:
              containerName: dind
              divisor: 1Gi
              resource: requests.memory
        - name: K8S_DIND_LIMITS_MEMORY_GI
          valueFrom:
            resourceFieldRef:
              containerName: dind
              divisor: 1Gi
              resource: limits.memory
        - name: K8S_DOCKER_GC_REQUESTS_CPU_M
          valueFrom:
            resourceFieldRef:
              containerName: docker-gc
              divisor: 1m
              resource: requests.cpu
        - name: K8S_DOCKER_GC_LIMITS_CPU_M
          valueFrom:
            resourceFieldRef:
              containerName: docker-gc
              divisor: 1m
              resource: limits.cpu
        - name: K8S_DOCKER_GC_REQUESTS_MEMORY_MI
          valueFrom:
            resourceFieldRef:
              containerName: docker-gc
              divisor: 1Mi
              resource: requests.memory
        - name: K8S_DOCKER_GC_LIMITS_MEMORY_MI
          valueFrom:
            resourceFieldRef:
              containerName: docker-gc
              divisor: 1Mi
              resource: limits.memory
        image: lsstsqre/jenkins-swarm-client:3.15-ldfc
        imagePullPolicy: Always
        name: swarm
        #command: ["/bin/sh", "-ec", "while :; do echo '.'; sleep 60 ; done"]
        readinessProbe:
          exec:
            command: [ 'wget', '-Y', 'off', '--spider', '-q', 'http://127.0.0.1:8080/metrics' ]
          failureThreshold: 2
          initialDelaySeconds: 5
          periodSeconds: 5
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          limits:
            cpu: "1"
            memory: 2Gi
          requests:
            cpu: "1"
            memory: 2Gi
        securityContext:
          allowPrivilegeEscalation: true
          privileged: false
          readOnlyRootFilesystem: false
          runAsGroup: 202
          runAsNonRoot: false
          runAsUser: 48435
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /j
          name: ws
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext:
        fsGroup: 4085
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
      shareProcessNamespace: false
      terminationGracePeriodSeconds: 30
      volumes:
      - emptyDir: {}
        name: docker-graph-storage
  updateStrategy:
    type: RollingUpdate
  volumeClaimTemplates:
  - apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: ws
      namespace: default
    spec:
      accessModes:
      - ReadWriteMany
      resources:
        requests:
          storage: "1500Gi"
      volumeMode: Filesystem
